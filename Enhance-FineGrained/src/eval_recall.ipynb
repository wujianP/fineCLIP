{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from processing_image import Preprocess\n",
    "# from modeling_frcnn import GeneralizedRCNN\n",
    "# from utils import Config\n",
    "# from transformers import LxmertTokenizer, LxmertForPreTraining\n",
    "import os, json\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from training.data import NpyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=\"/home/mila/l/le.zhang/scratch/open_clip/src/Outputs/negclip/checkpoints/epoch_0.pt\"\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model, _, image_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained=\"openai\",device=device)\n",
    "model, _, image_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained=checkpoint, device=device)\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging 5 splied files from /home/mila/l/le.zhang/scratch/winonground/data/processed_dataset/coco_val\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_path=\"/home/mila/l/le.zhang/scratch/winonground/data/processed_dataset/coco_val\"\n",
    "eval_dataset=NpyDataset(eval_dataset_path,image_preprocess,1000,tokenizer)\n",
    "eval_dataloader=DataLoader(eval_dataset,batch_size=256,shuffle=False,num_workers=0)\n",
    "len(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:11<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "gt_caption_features=[]\n",
    "hn_caption_features=[]\n",
    "all_image_features=[]\n",
    "for i in tqdm(eval_dataloader):\n",
    "    gt_caption=i[1][:,0,:].to(device)\n",
    "    hn_caption=i[1][:,1::].reshape(-1,77).to(device)\n",
    "    \n",
    "    images=i[0].to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        \n",
    "        image_features = model.encode_image(images)\n",
    "        gt_text_features = model.encode_text(gt_caption)\n",
    "        hn_text_features = model.encode_text(hn_caption)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        gt_text_features /= gt_text_features.norm(dim=-1, keepdim=True)\n",
    "        hn_text_features /= hn_text_features.norm(dim=-1, keepdim=True)\n",
    "        gt_caption_features.append(gt_text_features)\n",
    "        hn_caption_features.append(hn_text_features)\n",
    "        all_image_features.append(image_features)\n",
    "all_caption_features=torch.cat((torch.cat(gt_caption_features),torch.cat(hn_caption_features)))\n",
    "all_image_features=torch.cat(all_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=torch.load(\"/home/mila/l/le.zhang/scratch/open_clip/src/itr_results/saved_features/rank_coco-dis_text_mean-hn--5e-06-weightd0.5-weightr0.2-ub5-w_special_epoch_1_features.pt\")\n",
    "caption_features=features[\"caption_features\"].detach().cpu().numpy()\n",
    "image_features=features[\"image_features\"].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "np.savez_compressed(\"/home/mila/l/le.zhang/scratch/open_clip/src/itr_results/saved_features/rank_coco-dis_text_mean-hn--5e-06-weightd0.5-weightr0.2-ub5-w_special_epoch_1_features.npz\",caption_features=caption_features,image_features=image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def compute_similarity(image_features, text_features, bs = 500):\n",
    "    # compute similarity\n",
    "    dim0 = image_features.shape[0]\n",
    "    dim1 = text_features.shape[0]\n",
    "    similarity_scores = torch.zeros(dim0, dim1)\n",
    "    for v in range(0, dim0, bs):\n",
    "        for t in range(0, dim1, bs):\n",
    "            print('Processing Visual '+str(v)+' Text '+str(t), end='\\r')\n",
    "            batch_visual_emb = image_features[v:v+bs].cuda()\n",
    "            batch_caption_emb = text_features[t:t+bs].cuda()\n",
    "            with torch.no_grad():\n",
    "                logits = batch_visual_emb @ batch_caption_emb.t()\n",
    "                logits.cpu()caption_features\n",
    "            similarity_scores[v:v+bs,t:t+bs] = logits\n",
    "            # clear torch cuda cache\n",
    "            torch.cuda.empty_cache()\n",
    "            del batch_visual_emb, batch_caption_emb, logits\n",
    "\n",
    "\n",
    "    print('Done similarity')\n",
    "    return similarity_scores\n",
    "compute_similarity(image_features,caption_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "logits_per_image = (model.logit_scale * all_image_features @ all_caption_features.t()).detach().cpu()\n",
    "logits = {\"image_to_text\": logits_per_image}\n",
    "ground_truth = torch.arange(len(all_image_features)).view(-1, 1)\n",
    "\n",
    "for name, logit in logits.items():\n",
    "    ranking = torch.argsort(logit, descending=True)\n",
    "    preds = torch.where(ranking == ground_truth)[1]\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "    metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "    for k in [1, 5, 10]:\n",
    "        metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done similarityal 9500 Text 49500\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "clip_features=torch.load(\"/home/mila/l/le.zhang/scratch/open_clip/src/itr_results/saved_features/openai_pretrainedtest_features.pt\")\n",
    "\n",
    "\n",
    "# test time to calculate cosine similarity between a and b\n",
    "import time\n",
    "start=time.time()\n",
    "\n",
    "metrics = {}\n",
    "image_features=clip_features['image_features']\n",
    "text_features=clip_features['caption_features']\n",
    "logits_per_image = compute_similarity(image_features,text_features)\n",
    "\n",
    "logits = {\"image_to_text\": logits_per_image}\n",
    "ground_truth = torch.arange(len(image_features)).view(-1, 1)\n",
    "\n",
    "for name, logit in logits.items():\n",
    "    ranking = torch.argsort(logit, descending=True)\n",
    "    preds = torch.where(ranking == ground_truth)[1]\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "    metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "    for k in [1, 5, 10]:\n",
    "        metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "end=time.time()\n",
    "print(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 512])\n",
      "torch.Size([10000, 512])\n"
     ]
    }
   ],
   "source": [
    "# read from file\n",
    "import torch\n",
    "import numpy as np\n",
    "features=torch.load(\"/home/mila/l/le.zhang/scratch/open_clip/src/itr_results/saved_features/rank_coco-dis_text_mean-hn--5e-06-weightd0.5-weightr0.2-ub5-w_special_epoch_1_features.pt\")\n",
    "text_features=features[\"caption_features\"][:50000]\n",
    "image_features=features[\"image_features\"][:10000]\n",
    "print(text_features.shape)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_num=50000\n",
    "text_features=torch.cat(text_features[:subset_num],text_features[image_features.shape[0]:image_features.shape[0]+subset_num*4])\n",
    "image_features=image_features[:subset_num]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:20,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def compute_retrieval(a2b_sims, return_ranks=True ,start_index=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a2b_sims: Result of computing similarity between two sets of embeddings (emb1 @ emb2.T)\n",
    "            with shape (num_datapoints, num_datapoints).\n",
    "\n",
    "    Returns:\n",
    "        Retrieval metrics for that similarity.\n",
    "    \"\"\"\n",
    "    npts = a2b_sims.shape[0]\n",
    "    ranks = torch.zeros(npts)\n",
    "    # loop source embedding indices\n",
    "    for index in tqdm(range(start_index,npts+start_index)):\n",
    "        ranking = torch.argsort(a2b_sims, descending=True)\n",
    "        preds = torch.where(ranking == index)[1]\n",
    "        preds = preds.detach().cpu()\n",
    "      \n",
    "        ranks[index] = preds\n",
    "    return ranks\n",
    "\n",
    "bs=1000\n",
    "split_num=50\n",
    "image_len = image_features.shape[0]\n",
    "text_len  = text_features.shape[0]\n",
    "split_size=image_len//split_num\n",
    "total_ranks=[]\n",
    "for eid,partial_index in tqdm(enumerate(range(0, image_len, split_size))):\n",
    "    partial_image_features = image_features[partial_index:partial_index+split_size]\n",
    "    max_pairs=partial_image_features.shape[0]\n",
    "    sub_similarity_scores = torch.zeros((max_pairs, text_len))\n",
    "    for v in range(0, max_pairs, bs):\n",
    "        for t in range(0, text_len, bs):\n",
    "            batch_visual_emb = partial_image_features[v:v+bs]\n",
    "            batch_caption_emb = text_features[t:t+bs]\n",
    "            with torch.no_grad():\n",
    "                logits = batch_visual_emb @ batch_caption_emb.t()\n",
    "            sub_similarity_scores[v:v+bs,t:t+bs] = logits.detach().cpu()\n",
    "    a2b_sims=sub_similarity_scores.detach().cpu()\n",
    "    start_index=eid*split_size\n",
    "    npts = a2b_sims.shape[0]\n",
    "\n",
    "    # loop source embedding indices\n",
    "    # for index in tqdm(range(start_index,npts+start_index)):\n",
    "    #     ranking = torch.argsort(a2b_sims, descending=True)\n",
    "    #     preds = torch.where(ranking == index)[1]\n",
    "    #     preds = preds.detach().cpu()\n",
    "    #     ranks.append(preds)\n",
    "\n",
    "    ground_truth=torch.arange(eid*split_size,min((eid+1)*split_size,image_len)).reshape(-1,1)\n",
    "    ranking = torch.argsort(a2b_sims, descending=True) \n",
    "    preds = torch.where(ranking == ground_truth)[1]\n",
    "    # rank,top1 = compute_retrieval(sub_similarity_scores.detach().cpu(),eid*split_size)\n",
    "    total_ranks.append(preds)\n",
    "metrics={}\n",
    "total_ranks=torch.cat(total_ranks)\n",
    "def mean_of_list(l):\n",
    "    return round((sum(l)/len(l)).item(),2)\n",
    "name=\"clip\"\n",
    "for k in [1, 5, 10]:\n",
    "    metrics[f\"{name}_R@{k}\"] = mean_of_list(total_ranks < k)\n",
    "metrics[f\"{name}_mean_rank\"] = round(total_ranks.mean(dtype=torch.float32).item())+ 1\n",
    "metrics[f\"{name}_median_rank\"] = np.floor(np.median(total_ranks)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_R@1': 0.06,\n",
       " 'clip_R@5': 0.19,\n",
       " 'clip_R@10': 0.26,\n",
       " 'clip_mean_rank': 440,\n",
       " 'clip_median_rank': 57.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
